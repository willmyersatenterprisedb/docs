---
title: 'Red Hat OpenShift'
originalFilePath: 'src/openshift.md'
---

EDB Postgres Distributed for Kubernetes is a certified operator that can be
installed on OpenShift via the web interface.

## Ensuring access to EDB private registry

!!! Important
    You'll need access to the private EDB repository where both the operator
    and operand images are stored. Access requires a valid
    [EDB subscription plan](https://www.enterprisedb.com/products/plans-comparison).
    Please refer to ["Accessing EDB private image registries"](private_registries.md) for further details.

The OpenShift install will use pull secrets in order to access the
operand and operator images, which are held in a private repository.

Once you have credentials to the private repo, you will need to create
two pull secrets in the `openshift-operators` namespace, named:

-   `pgd-operator-pull-secret`, for the EDB Postgres Distributed for Kubernetes operator images
-   `postgresql-operator-pull-secret`, for the EDB Postgres for Kubernetes operator images

You can create each secret via the `oc create` command, as follows:

```sh
oc create secret docker-registry pgd-operator-pull-secret \
  -n openshift-operators --docker-server=docker.enterprisedb.com \
  --docker-username="@@REPOSITORY@@" \
  --docker-password="@@TOKEN@@"

oc create secret docker-registry postgresql-operator-pull-secret \
  -n openshift-operators --docker-server=docker.enterprisedb.com \
  --docker-username="@@REPOSITORY@@" \
  --docker-password="@@TOKEN@@"
```

where:

-   `@@REPOSITORY@@` is the name of the repository, as explained in ["Which repository to
    choose?"](private_registries.md#which-repository-to-choose)
-   `@@TOKEN@@` is the repository token for your EDB account, as explained in
    ["How to retrieve the token"](private_registries.md#how-to-retrieve-the-token)

## Installing the operator

The EDB Postgres Distributed for Kubernetes operator can be found in the Red
Hat OperatorHub directly from your OpenShift dashboard.

1.  Navigate in the web console to the `Operators -> OperatorHub` page:

    ![Menu OperatorHub](./images/openshift/operatorhub_1.png)

2.  Use the search box to restrict the listing, e.g. using `EDB` or `pgd`:

    ![Install OperatorHub](./images/openshift/find-pgd-openshift.png)

3.  Read the information about the Operator and select `Install`

4.  The following `Operator installation` page expects you to choose:

    -   the installation mode ([cluster-wide](#cluster-wide-installation) is the
        only mode at the moment)
    -   the update channel (at the moment `preview`)
    -   the approval strategy, following the availability on the market place of
        a new release of the operator, certified by Red Hat:
        -   `Automatic`: OLM automatically upgrades the running operator with the
            new version
        -   `Manual`:  OpenShift waits for human intervention, by requiring an
            approval in the `Installed Operators` section

### Cluster-wide installation

With cluster-wide installation, you are asking OpenShift to install the
Operator in the default `openshift-operators` namespace and to make it
available to all the projects in the cluster.

This is the default and normally recommended approach to install EDB Postgres
Distributed for Kubernetes.

From the web console, select `All namespaces on the cluster (default)` as
`Installation mode`:

![Install all namespaces](./images/openshift/all-namespaces.png)

On installation, the operator will be visible in all namespaces. In case there
were problems during installation, check the logs in any pods in the
`openshift-operators` project on the `Workloads → Pods` page,
as you would with any other OpenShift operator.

!!! Important "Beware"
    By choosing the cluster-wide installation you cannot easily move to a
    single project installation at a later time.

## Creating a PGD cluster

After the installation from OpenShift, you should find the operator deployment
in the `openshift-operators` namespace. Notice the cert-manager operator will
also get installed, as will the EDB Postgres for Kubernetes operator
(`postgresql-operator-controller-manager`).

```sh
$ oc get deployments -n openshift-operators
NAME                                            READY   UP-TO-DATE   AVAILABLE   AGE
cert-manager-operator                           1/1     1            1           11m
pgd-operator-controller-manager                 1/1     1            1           11m
postgresql-operator-controller-manager-1-20-0   1/1     1            1           23h
…
```

Checking that the `pgd-operator-controller-manager` deployment is READY, we can
start creating PGD clusters. The EDB Postgres Distributed for Kubernetes
repository contains some useful sample files.

Remember to deploy your PGD clusters on a dedicated namespace/project. The
default namespace is reserved.

First then, you should create a new namespace, and deploy a
[self-signed certificate `Issuer`](https://raw.githubusercontent.com/EnterpriseDB/edb-postgres-for-kubernetes-charts/main/hack/samples/issuer-selfsigned.yaml)
in it:

```sh
oc create ns my-namespace
oc apply -n my-namespace -f \
  https://raw.githubusercontent.com/EnterpriseDB/edb-postgres-for-kubernetes-charts/main/hack/samples/issuer-selfsigned.yaml
```

### Using PGD in a single Openshift Cluster in a single region

Please see the following section for [multi-cluster and multi-region](#using-pgd-in-multiple-openshift-clusters-in-multiple-regions) deployments.

Now you can deploy a PGD cluster, for example a flexible 3-region, which
contains two data groups and a witness group. You can find the YAML manifest
in the file [`flexible_3regions.yaml`](../samples/flexible_3regions.yaml).

```sh
oc apply -f flexible_3regions.yaml -n my-namespace
```

You should start seeing your PGD groups come up:

```sh
$ oc get pgdgroups -n my-namespace
NAME       DATA INSTANCES   WITNESS INSTANCES   PHASE                PHASE DETAILS   AGE
region-a   2                1                   PGDGroup - Healthy                   23m
region-b   2                1                   PGDGroup - Healthy                   23m
region-c   0                1                   PGDGroup - Healthy                   23m
```

### Using PGD in multiple Openshift Clusters in multiple regions

In order to deploy PGD in multiple Openshift Clusters in multiple regions you must first establish a way for the
PGD Groups to communicate with each other. The recommended way of achieving this with multiple Openshift clusters is to use
[Submariner](https://submariner.io/getting-started/quickstart/openshift/). Configuring the connectivity is outside the
scope of this document, but once you have established connectivity between the Openshift Clusters you can deploy
PGD Groups synced with one another.

!!! Warning
    This example assumes you are deploying three PGD Groups, one in each Openshift
    Cluster, and that you have established connectivity between the Openshift Clusters using Submariner.

Similar to the [single cluster example](#using-pgd-in-a-single-openshift-cluster-in-a-single-region), we will create
two data PGD groups and one witness group. In contrast to that example,
each group will live in a different Openshift Cluster.

In addition to basic connectivity between the Openshift Clusters, you will need to ensure that each Openshift Cluster
contains a certificate authority that is trusted by the other Openshift Clusters. This is required for the PGD Groups
to communicate with each other.

The Openshift clusters can all use
the same certificate authority, or each cluster can have its own certificate
authority. Either way, it needs to be ensured that each Openshift cluster's
certificates trust the other Openshift clusters' certificate authorities.

For illustration, we are going to demo using a self-signed certificate
that has a single certificate authority used for all certificates on all our Openshift clusters.

In this demo we will refer to the Openshift clusters as `Openshift Cluster A`, `Openshift Cluster B`, and
`Openshift Cluster C` . In Openshift, an installation of the PG4K-PGD-Operator from OperatorHub will include an
installation of the *cert-manager* operator; creating and managing certificates with cert-manager is
recommended. We create a namespace to hold `Openshift Cluster A`,  and in it
we will also create the needed objects for a self-signed certificate. Assuming
that the PGD operator and the cert-manager are installed, we create a [self-signed certificate `Issuer`](https://raw.githubusercontent.com/EnterpriseDB/edb-postgres-for-kubernetes-charts/main/hack/samples/issuer-selfsigned.yaml)
in that namespace.

```sh
oc create ns pgd-group
oc apply -n pgd-group -f \
  https://raw.githubusercontent.com/EnterpriseDB/edb-postgres-for-kubernetes-charts/main/hack/samples/issuer-selfsigned.yaml
```

After a few moments, cert-manager should have created the Issuers and Certificates. Additionally, there should now be
two secrets in the `pgd-group` namespace: `server-ca-key-pair` and `client-ca-key-pair`. These secrets contain
the certificates and private keys for the server and client certificate authorities. We will need to copy these secrets
to the other Openshift Clusters **before applying** the `issuer-selfsigned.yaml` manifest. We can use the
`oc get secret` command to get the contents of the secrets.

```sh
oc get secret server-ca-key-pair -n pgd-group -o yaml > server-ca-key-pair.yaml
oc get secret client-ca-key-pair -n pgd-group -o yaml > client-ca-key-pair.yaml
```

After removing the content specific to `Openshift Cluster A`
from the above secrets (such as uid, resourceVersion and timestamp,) we can switch our
context to `Openshift Cluster B`; we create the namespace, create our
secrets in it, and only then apply the `issuer-selfsigned.yaml` file.

```sh
oc create ns pgd-group
oc apply -n pgd-group -f server-ca-key-pair.yaml
oc apply -n pgd-group -f client-ca-key-pair.yaml
oc apply -n pgd-group -f \
  https://raw.githubusercontent.com/EnterpriseDB/edb-postgres-for-kubernetes-charts/main/hack/samples/issuer-selfsigned.yaml
```

Finally, we can switch our context to `Openshift Cluster C`, and repeat
the same process we followed for Cluster B.

```sh
oc create ns pgd-group
oc apply -n pgd-group -f server-ca-key-pair.yaml
oc apply -n pgd-group -f client-ca-key-pair.yaml
oc apply -n pgd-group -f \
  https://raw.githubusercontent.com/EnterpriseDB/edb-postgres-for-kubernetes-charts/main/hack/samples/issuer-selfsigned.yaml
```

Now, back on `Openshift Cluster A`, we can create our first PGD Group, called `region-a`. The YAML manifest for the PGD Group is as
follows:

```yaml
apiVersion: pgd.k8s.enterprisedb.io/v1beta1
kind: PGDGroup
metadata:
  name: region-a
spec:
  instances: 2
  proxyInstances: 2
  witnessInstances: 1
  pgd:
    parentGroup:
      name: world
      create: true
    discovery:
      - host: region-a-group.pgd-group.svc.clusterset.local
      - host: region-b-group.pgd-group.svc.clusterset.local
      - host: region-c-group.pgd-group.svc.clusterset.local
  cnp:
    storage:
      size: 1Gi
  connectivity:
    dns:
      domain: "pgd-group.svc.clusterset.local"
      additional:
        - domain: alternate.domain
        - domain: my.domain
          hostSuffix: -dc1
    tls:
      mode: verify-ca
      clientCert:
        caCertSecret: client-ca-key-pair
        certManager:
          spec:
            issuerRef:
              name: client-ca-issuer
              kind: Issuer
              group: cert-manager.io
      serverCert:
        caCertSecret: server-ca-key-pair
        certManager:
          spec:
            issuerRef:
              name: server-ca-issuer
              kind: Issuer
              group: cert-manager.io
```

!!! Important
    Please note that the format of the hostnames in the `discovery` section differs from the single cluster
    example. This is because we are using Submariner to connect the Openshift Clusters, and Submariner uses the
    `<service>.<ns>.svc.clusterset.local` domain to route traffic between the Openshift Clusters. `region-a-group` is the
    name of the service that will be created for the PGD Group named `region-a`.

Let's apply the `region-a` PGD Group YAML:

```sh
oc apply -f region-a.yaml -n pgd-group
```

We can now switch our context to `Openshift Cluster B` and create our second PGD Group. The YAML for the PGD Group in Cluster B
is as follows, the only difference is the `metadata.name`:

```yaml
apiVersion: pgd.k8s.enterprisedb.io/v1beta1
kind: PGDGroup
metadata:
  name: region-b
spec:
  instances: 2
  proxyInstances: 2
  witnessInstances: 1
  pgd:
    parentGroup:
      name: world
    discovery:
      - host: region-a-group.pgd-group.svc.clusterset.local
      - host: region-b-group.pgd-group.svc.clusterset.local
      - host: region-c-group.pgd-group.svc.clusterset.local
  cnp:
    storage:
      size: 1Gi
  connectivity:
    dns:
      domain: "pgd-group.svc.clusterset.local"
    tls:
      mode: verify-ca
      clientCert:
        caCertSecret: client-ca-key-pair
        certManager:
          spec:
            issuerRef:
              name: client-ca-issuer
              kind: Issuer
              group: cert-manager.io
      serverCert:
        caCertSecret: server-ca-key-pair
        certManager:
          spec:
            issuerRef:
              name: server-ca-issuer
              kind: Issuer
              group: cert-manager.io
```

Apply the `region-b` PGD Group YAML:

```sh
oc apply -f region-b.yaml -n pgd-group
```

And finally, we can switch our context to `Openshift Cluster C` and create our third PGD Group. The YAML for the PGD
Group is as follows:

```yaml
apiVersion: pgd.k8s.enterprisedb.io/v1beta1
kind: PGDGroup
metadata:
  name: region-c
spec:
  instances: 0
  proxyInstances: 0
  witnessInstances: 1
  pgd:
    parentGroup:
      name: world
    discovery:
      - host: region-a-group.pgd-group.svc.clusterset.local
      - host: region-b-group.pgd-group.svc.clusterset.local
      - host: region-c-group.pgd-group.svc.clusterset.local
  cnp:
    storage:
      size: 1Gi
  connectivity:
    dns:
      domain: "pgd-group.svc.clusterset.local"
    tls:
      mode: verify-ca
      clientCert:
        caCertSecret: client-ca-key-pair
        certManager:
          spec:
            issuerRef:
              name: client-ca-issuer
              kind: Issuer
              group: cert-manager.io
      serverCert:
        caCertSecret: server-ca-key-pair
        certManager:
          spec:
            issuerRef:
              name: server-ca-issuer
              kind: Issuer
              group: cert-manager.io
```

Apply the `region-c` PGD Group YAML:

```sh
oc apply -f region-c.yaml -n pgd-group
```

Now we can switch our context back to `Openshift Cluster A` and check the status of our PGD Group there.

```sh
oc get pgdgroup region-a -n pgd-group
```

We should expect to find the PGD group in phase
`PGD - Waiting for node discovery`.

After creating the PGD Groups in each Openshift Cluster, which will in turn create the services for each node, you will
need to expose the services to the other Openshift Clusters. This can be done in various ways.
Since we are using
Submariner, we will do it using the
[`subctl`](https://submariner.io/operations/deployment/subctl/)
command. We need to run the `subctl export service` command
for each service in our
`pgd-group` namespace that has a `-group` or `-node` suffix. We can accomplish this by running the following bash
`for` loop on each cluster:

```sh
for service in $(oc get svc -n pgd-group --no-headers -o custom-columns="NAME:.metadata.name" | grep -E '(-group|-node)$'); do
  subctl export service $service -n pgd-group
done
```

After a few minutes the status should show that the PGD Group is healthy. Once each PGD Group is healthy, you can write
to the `app` database in either of the two data nodes, `region-a` or `region-b`, and the data will be replicated to the
other data node.

<!--
## Upgrades

TODO
-->